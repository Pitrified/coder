{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from haystack import Document, component\n",
    "from loguru import logger as lg\n",
    "from rich import print as rprint\n",
    "\n",
    "from coder.config.coder_config import CoderConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CoderConfig()\n",
    "rprint(cc.paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeFileDocumentExtractor:\n",
    "    \"\"\"\n",
    "    Tool class that processes a single Python file and splits the code into several chunks.\n",
    "    Chunks are based on:\n",
    "      - Top-level functions,\n",
    "      - Top-level classes,\n",
    "      - Methods inside classes,\n",
    "      - And code outside these definitions (global code).\n",
    "\n",
    "    For each chunk, the following metadata is saved:\n",
    "      - 'chunk_number': Sequential number of the chunk.\n",
    "      - 'first_line': The first line of code in the chunk.\n",
    "      - 'start_line' and 'end_line': The boundaries in the original file.\n",
    "      - 'type': Type of the chunk (e.g. 'global', 'function', 'class', 'method').\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: Path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def _parse_code(self, code: str) -> ast.AST:\n",
    "        \"\"\"Parses the source code into an AST.\"\"\"\n",
    "        return ast.parse(code)\n",
    "\n",
    "    def _get_top_level_defs(\n",
    "        self, tree: ast.AST\n",
    "    ) -> list[tuple[int, int, str, str, ast.AST]]:\n",
    "        \"\"\"\n",
    "        Returns a list of top-level definitions as tuples:\n",
    "        (start_line, end_line, type, name, node)\n",
    "        \"\"\"\n",
    "        defs = []\n",
    "        for node in tree.body:\n",
    "            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "                defs.append(\n",
    "                    (\n",
    "                        node.lineno,\n",
    "                        getattr(node, \"end_lineno\", node.lineno),\n",
    "                        \"function\",\n",
    "                        node.name,\n",
    "                        node,\n",
    "                    )\n",
    "                )\n",
    "            elif isinstance(node, ast.ClassDef):\n",
    "                defs.append(\n",
    "                    (\n",
    "                        node.lineno,\n",
    "                        getattr(node, \"end_lineno\", node.lineno),\n",
    "                        \"class\",\n",
    "                        node.name,\n",
    "                        node,\n",
    "                    )\n",
    "                )\n",
    "        return sorted(defs, key=lambda x: x[0])\n",
    "\n",
    "    def _extract_global_intervals(\n",
    "        self,\n",
    "        code_lines: list[str],\n",
    "        top_level_defs: list[tuple[int, int, str, str, ast.AST]],\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Determines intervals (line ranges) for global code segments that are not covered by any\n",
    "        top-level definition.\n",
    "        \"\"\"\n",
    "        intervals = []\n",
    "        last_end = 1\n",
    "        for start, end, typ, name, node in top_level_defs:\n",
    "            if start > last_end:\n",
    "                intervals.append((last_end, start - 1, {\"type\": \"global\"}))\n",
    "            last_end = max(last_end, end + 1)\n",
    "        if last_end <= len(code_lines):\n",
    "            intervals.append((last_end, len(code_lines), {\"type\": \"global\"}))\n",
    "        return intervals\n",
    "\n",
    "    def _process_function(\n",
    "        self, start: int, end: int, name: str\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"Returns the interval for a function.\"\"\"\n",
    "        return [(start, end, {\"type\": \"function\", \"name\": name})]\n",
    "\n",
    "    def _process_class(\n",
    "        self, class_node: ast.ClassDef\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Processes a class node, splitting it into:\n",
    "          - Header (code before the first method),\n",
    "          - Method intervals,\n",
    "          - Gaps between methods (middle), and\n",
    "          - Footer (code after the last method).\n",
    "        \"\"\"\n",
    "        intervals = []\n",
    "        class_start = class_node.lineno\n",
    "        class_end = getattr(class_node, \"end_lineno\", class_node.lineno)\n",
    "        methods = []\n",
    "        for child in class_node.body:\n",
    "            if isinstance(child, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "                methods.append(\n",
    "                    (\n",
    "                        child.lineno,\n",
    "                        getattr(child, \"end_lineno\", child.lineno),\n",
    "                        \"method\",\n",
    "                        child.name,\n",
    "                    )\n",
    "                )\n",
    "        methods.sort(key=lambda x: x[0])\n",
    "        if methods:\n",
    "            # Class header: before the first method.\n",
    "            if class_start < methods[0][0]:\n",
    "                intervals.append(\n",
    "                    (\n",
    "                        class_start,\n",
    "                        methods[0][0] - 1,\n",
    "                        {\"type\": \"class\", \"name\": class_node.name, \"part\": \"header\"},\n",
    "                    )\n",
    "                )\n",
    "            prev_end = methods[0][1]\n",
    "            intervals.append(\n",
    "                (\n",
    "                    methods[0][0],\n",
    "                    methods[0][1],\n",
    "                    {\"type\": \"method\", \"class\": class_node.name, \"name\": methods[0][3]},\n",
    "                )\n",
    "            )\n",
    "            for i in range(1, len(methods)):\n",
    "                # Gap between methods.\n",
    "                if prev_end + 1 < methods[i][0]:\n",
    "                    intervals.append(\n",
    "                        (\n",
    "                            prev_end + 1,\n",
    "                            methods[i][0] - 1,\n",
    "                            {\n",
    "                                \"type\": \"class\",\n",
    "                                \"name\": class_node.name,\n",
    "                                \"part\": \"middle\",\n",
    "                            },\n",
    "                        )\n",
    "                    )\n",
    "                intervals.append(\n",
    "                    (\n",
    "                        methods[i][0],\n",
    "                        methods[i][1],\n",
    "                        {\n",
    "                            \"type\": \"method\",\n",
    "                            \"class\": class_node.name,\n",
    "                            \"name\": methods[i][3],\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "                prev_end = methods[i][1]\n",
    "            # Class footer: after the last method.\n",
    "            if prev_end < class_end:\n",
    "                intervals.append(\n",
    "                    (\n",
    "                        prev_end + 1,\n",
    "                        class_end,\n",
    "                        {\"type\": \"class\", \"name\": class_node.name, \"part\": \"footer\"},\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            intervals.append(\n",
    "                (class_start, class_end, {\"type\": \"class\", \"name\": class_node.name})\n",
    "            )\n",
    "        return intervals\n",
    "\n",
    "    def _extract_defs_intervals(\n",
    "        self, tree: ast.AST\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Processes all top-level definitions (functions and classes) and returns their intervals.\n",
    "        \"\"\"\n",
    "        intervals = []\n",
    "        top_level_defs = self._get_top_level_defs(tree)\n",
    "        for start, end, typ, name, node in top_level_defs:\n",
    "            if typ == \"function\":\n",
    "                intervals.extend(self._process_function(start, end, name))\n",
    "            elif typ == \"class\":\n",
    "                intervals.extend(self._process_class(node))\n",
    "        return intervals\n",
    "\n",
    "    def _create_documents_from_intervals(\n",
    "        self, code_lines: list[str], intervals: list[tuple[int, int, dict[str, Any]]]\n",
    "    ) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Converts each interval (chunk) into a Haystack Document with rich metadata.\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        chunk_number = 1\n",
    "        for start, end, meta in intervals:\n",
    "            if start > end or start < 1 or end > len(code_lines):\n",
    "                continue\n",
    "            chunk_lines = code_lines[start - 1 : end]  # Convert to 0-indexed.\n",
    "            content = \"\\n\".join(chunk_lines)\n",
    "            first_line = chunk_lines[0] if chunk_lines else \"\"\n",
    "            meta_updated = meta.copy()\n",
    "            meta_updated.update(\n",
    "                {\n",
    "                    \"chunk_number\": chunk_number,\n",
    "                    \"first_line\": first_line,\n",
    "                    \"start_line\": start,\n",
    "                    \"end_line\": end,\n",
    "                }\n",
    "            )\n",
    "            # skip empty chunks\n",
    "            if not content.strip():\n",
    "                lines = f\"lines {start}-{end}\"\n",
    "                lg.debug(f\"Skipping empty chunk {chunk_number} ({lines})\")\n",
    "                # chunk_number += 1\n",
    "                continue\n",
    "            documents.append(Document(content=content, meta=meta_updated))\n",
    "            chunk_number += 1\n",
    "        return documents\n",
    "\n",
    "    def extract_documents(self) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Orchestrates the extraction process:\n",
    "          1. Reads the file and splits it into lines.\n",
    "          2. Parses the code into an AST.\n",
    "          3. Extracts intervals for definitions and global code.\n",
    "          4. Converts intervals into Haystack Documents.\n",
    "        \"\"\"\n",
    "        code = self.file_path.read_text(encoding=\"utf-8\")\n",
    "        code_lines = code.splitlines()\n",
    "        try:\n",
    "            tree = self._parse_code(code)\n",
    "        except Exception as e:\n",
    "            # If parsing fails, return the whole file as a single chunk with an error.\n",
    "            return [\n",
    "                Document(\n",
    "                    content=code,\n",
    "                    meta={\n",
    "                        \"error\": str(e),\n",
    "                        \"chunk_number\": 1,\n",
    "                        \"first_line\": code_lines[0] if code_lines else \"\",\n",
    "                        \"start_line\": 1,\n",
    "                        \"end_line\": len(code_lines),\n",
    "                    },\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        # Get intervals for definitions and global code.\n",
    "        defs_intervals = self._extract_defs_intervals(tree)\n",
    "        top_level_defs = self._get_top_level_defs(tree)\n",
    "        global_intervals = self._extract_global_intervals(code_lines, top_level_defs)\n",
    "        intervals = defs_intervals + global_intervals\n",
    "        intervals.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "        # Create documents from intervals.\n",
    "        documents = self._create_documents_from_intervals(code_lines, intervals)\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class CodeFolderIngestor:\n",
    "    \"\"\"\n",
    "    Pipeline component that accepts a list of folder paths, uses the CodeFileDocumentExtractor\n",
    "    for each Python file found in each folder, and aggregates the resulting Haystack Documents.\n",
    "    \"\"\"\n",
    "\n",
    "    @component.output_types(documents=list[Document])\n",
    "    def run(self, folder_paths: list[Path]) -> dict[str, list[Document]]:\n",
    "        documents: list[Document] = []\n",
    "        for folder_path in folder_paths:\n",
    "            for file_path in folder_path.rglob(\"*.py\"):\n",
    "                extractor = CodeFileDocumentExtractor(file_path)\n",
    "                document = extractor.extract_documents()\n",
    "                documents.extend(document)\n",
    "        return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [cc.paths.src_fol]\n",
    "ingestor = CodeFolderIngestor()\n",
    "result = ingestor.run(folders)\n",
    "documents = result[\"documents\"]\n",
    "print(f\"Ingested {len(documents)} documents from the provided folder paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].content[:100], documents[2].meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_integrations.document_stores.chroma import ChromaDocumentStore\n",
    "\n",
    "document_store = ChromaDocumentStore(\n",
    "    persist_path=str(cc.paths.chroma_fol),\n",
    ")\n",
    "document_store.write_documents(\n",
    "    documents=documents,\n",
    ")\n",
    "print(document_store.count_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample retriever\n",
    "from haystack_integrations.components.retrievers.chroma import ChromaEmbeddingRetriever\n",
    "\n",
    "retriever = ChromaEmbeddingRetriever(document_store=document_store)\n",
    "\n",
    "# example run query\n",
    "retriever.run(query_embedding=[0.1] * 384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
