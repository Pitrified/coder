{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from haystack import Document, component\n",
    "from loguru import logger as lg\n",
    "from rich import print as rprint\n",
    "\n",
    "from coder.config.coder_config import CoderConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CoderConfig()\n",
    "rprint(cc.paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeFileDocumentExtractor:\n",
    "    \"\"\"\n",
    "    Tool class that processes a single Python file and splits the code into several chunks.\n",
    "    Chunks are based on:\n",
    "      - Top-level functions,\n",
    "      - Top-level classes,\n",
    "      - Methods inside classes,\n",
    "      - And code outside these definitions (global code).\n",
    "\n",
    "    For each chunk, the following metadata is saved:\n",
    "      - 'chunk_number': Sequential number of the chunk.\n",
    "      - 'first_line': The first line of code in the chunk.\n",
    "      - 'start_line' and 'end_line': The boundaries in the original file.\n",
    "      - 'type': Type of the chunk (e.g. 'global', 'function', 'class', 'method').\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: Path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def _parse_code(self, code: str) -> ast.AST:\n",
    "        \"\"\"Parses the source code into an AST.\"\"\"\n",
    "        return ast.parse(code)\n",
    "\n",
    "    def _get_top_level_defs(\n",
    "        self, tree: ast.AST\n",
    "    ) -> list[tuple[int, int, str, str, ast.AST]]:\n",
    "        \"\"\"\n",
    "        Returns a list of top-level definitions as tuples:\n",
    "        (start_line, end_line, type, name, node)\n",
    "        \"\"\"\n",
    "        defs = []\n",
    "        for node in tree.body:\n",
    "            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "                defs.append(\n",
    "                    (\n",
    "                        node.lineno,\n",
    "                        getattr(node, \"end_lineno\", node.lineno),\n",
    "                        \"function\",\n",
    "                        node.name,\n",
    "                        node,\n",
    "                    )\n",
    "                )\n",
    "            elif isinstance(node, ast.ClassDef):\n",
    "                defs.append(\n",
    "                    (\n",
    "                        node.lineno,\n",
    "                        getattr(node, \"end_lineno\", node.lineno),\n",
    "                        \"class\",\n",
    "                        node.name,\n",
    "                        node,\n",
    "                    )\n",
    "                )\n",
    "        return sorted(defs, key=lambda x: x[0])\n",
    "\n",
    "    def _extract_global_intervals(\n",
    "        self,\n",
    "        code_lines: list[str],\n",
    "        top_level_defs: list[tuple[int, int, str, str, ast.AST]],\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Determines intervals (line ranges) for global code segments that are not covered by any\n",
    "        top-level definition.\n",
    "        \"\"\"\n",
    "        intervals = []\n",
    "        last_end = 1\n",
    "        for start, end, typ, name, node in top_level_defs:\n",
    "            if start > last_end:\n",
    "                intervals.append((last_end, start - 1, {\"type\": \"global\"}))\n",
    "            last_end = max(last_end, end + 1)\n",
    "        if last_end <= len(code_lines):\n",
    "            intervals.append((last_end, len(code_lines), {\"type\": \"global\"}))\n",
    "        return intervals\n",
    "\n",
    "    def _process_function(\n",
    "        self, start: int, end: int, name: str\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"Returns the interval for a function.\"\"\"\n",
    "        return [(start, end, {\"type\": \"function\", \"name\": name})]\n",
    "\n",
    "    def _process_class(\n",
    "        self, class_node: ast.ClassDef\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Processes a class node, splitting it into:\n",
    "          - Header (code before the first method),\n",
    "          - Method intervals,\n",
    "          - Gaps between methods (middle), and\n",
    "          - Footer (code after the last method).\n",
    "        \"\"\"\n",
    "        intervals = []\n",
    "        class_start = class_node.lineno\n",
    "        class_end = getattr(class_node, \"end_lineno\", class_node.lineno)\n",
    "        methods = []\n",
    "        for child in class_node.body:\n",
    "            if isinstance(child, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "                methods.append(\n",
    "                    (\n",
    "                        child.lineno,\n",
    "                        getattr(child, \"end_lineno\", child.lineno),\n",
    "                        \"method\",\n",
    "                        child.name,\n",
    "                    )\n",
    "                )\n",
    "        methods.sort(key=lambda x: x[0])\n",
    "        if methods:\n",
    "            # Class header: before the first method.\n",
    "            if class_start < methods[0][0]:\n",
    "                intervals.append(\n",
    "                    (\n",
    "                        class_start,\n",
    "                        methods[0][0] - 1,\n",
    "                        {\"type\": \"class\", \"name\": class_node.name, \"part\": \"header\"},\n",
    "                    )\n",
    "                )\n",
    "            prev_end = methods[0][1]\n",
    "            intervals.append(\n",
    "                (\n",
    "                    methods[0][0],\n",
    "                    methods[0][1],\n",
    "                    {\"type\": \"method\", \"class\": class_node.name, \"name\": methods[0][3]},\n",
    "                )\n",
    "            )\n",
    "            for i in range(1, len(methods)):\n",
    "                # Gap between methods.\n",
    "                if prev_end + 1 < methods[i][0]:\n",
    "                    intervals.append(\n",
    "                        (\n",
    "                            prev_end + 1,\n",
    "                            methods[i][0] - 1,\n",
    "                            {\n",
    "                                \"type\": \"class\",\n",
    "                                \"name\": class_node.name,\n",
    "                                \"part\": \"middle\",\n",
    "                            },\n",
    "                        )\n",
    "                    )\n",
    "                intervals.append(\n",
    "                    (\n",
    "                        methods[i][0],\n",
    "                        methods[i][1],\n",
    "                        {\n",
    "                            \"type\": \"method\",\n",
    "                            \"class\": class_node.name,\n",
    "                            \"name\": methods[i][3],\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "                prev_end = methods[i][1]\n",
    "            # Class footer: after the last method.\n",
    "            if prev_end < class_end:\n",
    "                intervals.append(\n",
    "                    (\n",
    "                        prev_end + 1,\n",
    "                        class_end,\n",
    "                        {\"type\": \"class\", \"name\": class_node.name, \"part\": \"footer\"},\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            intervals.append(\n",
    "                (class_start, class_end, {\"type\": \"class\", \"name\": class_node.name})\n",
    "            )\n",
    "        return intervals\n",
    "\n",
    "    def _extract_defs_intervals(\n",
    "        self, tree: ast.AST\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Processes all top-level definitions (functions and classes) and returns their intervals.\n",
    "        \"\"\"\n",
    "        intervals = []\n",
    "        top_level_defs = self._get_top_level_defs(tree)\n",
    "        for start, end, typ, name, node in top_level_defs:\n",
    "            if typ == \"function\":\n",
    "                intervals.extend(self._process_function(start, end, name))\n",
    "            elif typ == \"class\":\n",
    "                intervals.extend(self._process_class(node))\n",
    "        return intervals\n",
    "\n",
    "    def _create_documents_from_intervals(\n",
    "        self, code_lines: list[str], intervals: list[tuple[int, int, dict[str, Any]]]\n",
    "    ) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Converts each interval (chunk) into a Haystack Document with rich metadata.\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        chunk_number = 1\n",
    "        for start, end, meta in intervals:\n",
    "            if start > end or start < 1 or end > len(code_lines):\n",
    "                continue\n",
    "            chunk_lines = code_lines[start - 1 : end]  # Convert to 0-indexed.\n",
    "            content = \"\\n\".join(chunk_lines)\n",
    "            first_line = chunk_lines[0] if chunk_lines else \"\"\n",
    "            meta_updated = meta.copy()\n",
    "            meta_updated.update(\n",
    "                {\n",
    "                    \"chunk_number\": chunk_number,\n",
    "                    \"first_line\": first_line,\n",
    "                    \"start_line\": start,\n",
    "                    \"end_line\": end,\n",
    "                }\n",
    "            )\n",
    "            documents.append(Document(content=content, meta=meta_updated))\n",
    "            chunk_number += 1\n",
    "        return documents\n",
    "\n",
    "    def extract_documents(self) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Orchestrates the extraction process:\n",
    "          1. Reads the file and splits it into lines.\n",
    "          2. Parses the code into an AST.\n",
    "          3. Extracts intervals for definitions and global code.\n",
    "          4. Converts intervals into Haystack Documents.\n",
    "        \"\"\"\n",
    "        code = self.file_path.read_text(encoding=\"utf-8\")\n",
    "        code_lines = code.splitlines()\n",
    "        try:\n",
    "            tree = self._parse_code(code)\n",
    "        except Exception as e:\n",
    "            # If parsing fails, return the whole file as a single chunk with an error.\n",
    "            return [\n",
    "                Document(\n",
    "                    content=code,\n",
    "                    meta={\n",
    "                        \"error\": str(e),\n",
    "                        \"chunk_number\": 1,\n",
    "                        \"first_line\": code_lines[0] if code_lines else \"\",\n",
    "                        \"start_line\": 1,\n",
    "                        \"end_line\": len(code_lines),\n",
    "                    },\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        # Get intervals for definitions and global code.\n",
    "        defs_intervals = self._extract_defs_intervals(tree)\n",
    "        top_level_defs = self._get_top_level_defs(tree)\n",
    "        global_intervals = self._extract_global_intervals(code_lines, top_level_defs)\n",
    "        intervals = defs_intervals + global_intervals\n",
    "        intervals.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "        # Create documents from intervals.\n",
    "        documents = self._create_documents_from_intervals(code_lines, intervals)\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class CodeFolderIngestor:\n",
    "    \"\"\"\n",
    "    Pipeline component that accepts a list of folder paths, uses the CodeFileDocumentExtractor\n",
    "    for each Python file found in each folder, and aggregates the resulting Haystack Documents.\n",
    "    \"\"\"\n",
    "\n",
    "    @component.output_types(documents=list[Document])\n",
    "    def run(self, folder_paths: list[Path]) -> dict[str, list[Document]]:\n",
    "        documents: list[Document] = []\n",
    "        for folder_path in folder_paths:\n",
    "            for file_path in folder_path.rglob(\"*.py\"):\n",
    "                extractor = CodeFileDocumentExtractor(file_path)\n",
    "                document = extractor.extract_documents()\n",
    "                documents.extend(document)\n",
    "        return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [cc.paths.src_fol]\n",
    "ingestor = CodeFolderIngestor()\n",
    "result = ingestor.run(folders)\n",
    "documents = result[\"documents\"]\n",
    "print(f\"Ingested {len(documents)} documents from the provided folder paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].content[:100], documents[2].meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeFileDocumentExtractor:\n",
    "    \"\"\"\n",
    "    Tool class that processes a single Python file and splits the code into several chunks.\n",
    "    The splitting is based on:\n",
    "      - Top-level functions,\n",
    "      - Top-level classes,\n",
    "      - Methods inside classes,\n",
    "      - And any code outside these definitions (global code).\n",
    "\n",
    "    For each chunk, the following metadata is saved:\n",
    "      - 'chunk_number': Sequential number of the chunk.\n",
    "      - 'first_line': The first line of code in the chunk.\n",
    "      - 'start_line' and 'end_line': The boundaries in the original file.\n",
    "      - 'type': Type of the chunk (e.g. 'global', 'function', 'class', 'method').\n",
    "      - Other details such as the name of the function/class or, for class chunks, which part (header, middle, footer).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: Path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def extract_documents(self) -> list[Document]:\n",
    "        code = self.file_path.read_text(encoding=\"utf-8\")\n",
    "        code_lines = code.splitlines()\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "        except Exception as e:\n",
    "            # If the file fails to parse, return the whole file as one chunk with an error message.\n",
    "            return [\n",
    "                Document(\n",
    "                    content=code,\n",
    "                    meta={\n",
    "                        \"error\": str(e),\n",
    "                        \"chunk_number\": 1,\n",
    "                        \"first_line\": code_lines[0] if code_lines else \"\",\n",
    "                    },\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        # We'll accumulate intervals (start_line, end_line, metadata) for each chunk.\n",
    "        intervals = []\n",
    "\n",
    "        def add_interval(start: int, end: int, meta: dict[str, Any]) -> None:\n",
    "            if start <= end:\n",
    "                intervals.append((start, end, meta))\n",
    "\n",
    "        # === 1. Top-level definitions (functions & classes) ===\n",
    "        top_level_defs = []  # Each item: (start, end, type, name)\n",
    "        for node in tree.body:\n",
    "            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "                top_level_defs.append(\n",
    "                    (\n",
    "                        node.lineno,\n",
    "                        getattr(node, \"end_lineno\", node.lineno),\n",
    "                        \"function\",\n",
    "                        node.name,\n",
    "                    )\n",
    "                )\n",
    "            elif isinstance(node, ast.ClassDef):\n",
    "                top_level_defs.append(\n",
    "                    (\n",
    "                        node.lineno,\n",
    "                        getattr(node, \"end_lineno\", node.lineno),\n",
    "                        \"class\",\n",
    "                        node.name,\n",
    "                    )\n",
    "                )\n",
    "        top_level_defs.sort(key=lambda x: x[0])\n",
    "\n",
    "        # === 2. Global code: segments outside of any top-level definition ===\n",
    "        last_end = 1\n",
    "        for start, end, typ, name in top_level_defs:\n",
    "            if start > last_end:\n",
    "                add_interval(last_end, start - 1, {\"type\": \"global\"})\n",
    "            last_end = max(last_end, end + 1)\n",
    "        if last_end <= len(code_lines):\n",
    "            add_interval(last_end, len(code_lines), {\"type\": \"global\"})\n",
    "\n",
    "        # === 3. Process top-level definitions ===\n",
    "        for start, end, typ, name in top_level_defs:\n",
    "            if typ == \"function\":\n",
    "                add_interval(start, end, {\"type\": \"function\", \"name\": name})\n",
    "            elif typ == \"class\":\n",
    "                # Find the class node so we can inspect its body.\n",
    "                class_node = None\n",
    "                for node in tree.body:\n",
    "                    if (\n",
    "                        isinstance(node, ast.ClassDef)\n",
    "                        and node.name == name\n",
    "                        and node.lineno == start\n",
    "                    ):\n",
    "                        class_node = node\n",
    "                        break\n",
    "                if class_node is None:\n",
    "                    add_interval(start, end, {\"type\": \"class\", \"name\": name})\n",
    "                else:\n",
    "                    # For a class, also consider methods (only direct children)\n",
    "                    methods = []\n",
    "                    for child in class_node.body:\n",
    "                        if isinstance(child, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "                            methods.append(\n",
    "                                (\n",
    "                                    child.lineno,\n",
    "                                    getattr(child, \"end_lineno\", child.lineno),\n",
    "                                    \"method\",\n",
    "                                    child.name,\n",
    "                                )\n",
    "                            )\n",
    "                    methods.sort(key=lambda x: x[0])\n",
    "                    if methods:\n",
    "                        # Class header: from the start of the class to just before the first method.\n",
    "                        if class_node.lineno < methods[0][0]:\n",
    "                            add_interval(\n",
    "                                class_node.lineno,\n",
    "                                methods[0][0] - 1,\n",
    "                                {\"type\": \"class\", \"name\": name, \"part\": \"header\"},\n",
    "                            )\n",
    "                        # Process each method and gaps between methods.\n",
    "                        prev_end = methods[0][1]\n",
    "                        add_interval(\n",
    "                            methods[0][0],\n",
    "                            methods[0][1],\n",
    "                            {\"type\": \"method\", \"class\": name, \"name\": methods[0][3]},\n",
    "                        )\n",
    "                        for i in range(1, len(methods)):\n",
    "                            # Gap between previous method and current method, if any.\n",
    "                            if prev_end + 1 < methods[i][0]:\n",
    "                                add_interval(\n",
    "                                    prev_end + 1,\n",
    "                                    methods[i][0] - 1,\n",
    "                                    {\"type\": \"class\", \"name\": name, \"part\": \"middle\"},\n",
    "                                )\n",
    "                            add_interval(\n",
    "                                methods[i][0],\n",
    "                                methods[i][1],\n",
    "                                {\n",
    "                                    \"type\": \"method\",\n",
    "                                    \"class\": name,\n",
    "                                    \"name\": methods[i][3],\n",
    "                                },\n",
    "                            )\n",
    "                            prev_end = methods[i][1]\n",
    "                        # Class footer: from end of last method to the end of the class.\n",
    "                        if prev_end < class_node.end_lineno:\n",
    "                            add_interval(\n",
    "                                prev_end + 1,\n",
    "                                class_node.end_lineno,\n",
    "                                {\"type\": \"class\", \"name\": name, \"part\": \"footer\"},\n",
    "                            )\n",
    "                    else:\n",
    "                        add_interval(\n",
    "                            class_node.lineno,\n",
    "                            class_node.end_lineno,\n",
    "                            {\"type\": \"class\", \"name\": name},\n",
    "                        )\n",
    "\n",
    "        # === 4. Sort intervals by start line ===\n",
    "        intervals.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "        # === 5. Create documents for each interval ===\n",
    "        documents = []\n",
    "        chunk_number = 1\n",
    "        for start, end, meta in intervals:\n",
    "            if start > end or start < 1 or end > len(code_lines):\n",
    "                continue\n",
    "            chunk_lines = code_lines[start - 1 : end]  # converting to 0-indexed\n",
    "            content = \"\\n\".join(chunk_lines)\n",
    "            first_line = chunk_lines[0] if chunk_lines else \"\"\n",
    "            meta_updated = meta.copy()\n",
    "            meta_updated.update(\n",
    "                {\n",
    "                    \"chunk_number\": chunk_number,\n",
    "                    \"first_line\": first_line,\n",
    "                    \"start_line\": start,\n",
    "                    \"end_line\": end,\n",
    "                }\n",
    "            )\n",
    "            documents.append(Document(content=content, meta=meta_updated))\n",
    "            chunk_number += 1\n",
    "\n",
    "        return documents\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = Path(\"/home/pmn/repos/coder/src/coder/config/coder_paths.py\")\n",
    "    # Replace with your file\n",
    "    extractor = CodeFileDocumentExtractor(file_path)\n",
    "    chunks = extractor.extract_documents()\n",
    "    for doc in chunks:\n",
    "        print(\n",
    "            f\"Chunk {doc.meta['chunk_number']} (lines {doc.meta['start_line']}-{doc.meta['end_line']}):\"\n",
    "        )\n",
    "        print(f\"First line: {doc.meta['first_line']}\")\n",
    "        print(f\"Content:\\n{doc.content}\")\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeFileDocumentExtractor:\n",
    "    \"\"\"\n",
    "    Tool class that processes a single Python file and splits the code into several chunks.\n",
    "    Chunks are based on:\n",
    "      - Top-level functions,\n",
    "      - Top-level classes,\n",
    "      - Methods inside classes,\n",
    "      - And code outside these definitions (global code).\n",
    "\n",
    "    For each chunk, the following metadata is saved:\n",
    "      - 'chunk_number': Sequential number of the chunk.\n",
    "      - 'first_line': The first line of code in the chunk.\n",
    "      - 'start_line' and 'end_line': The boundaries in the original file.\n",
    "      - 'type': Type of the chunk (e.g. 'global', 'function', 'class', 'method').\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: Path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def _parse_code(self, code: str) -> ast.AST:\n",
    "        \"\"\"Parses the source code into an AST.\"\"\"\n",
    "        return ast.parse(code)\n",
    "\n",
    "    def _get_top_level_defs(\n",
    "        self, tree: ast.AST\n",
    "    ) -> list[tuple[int, int, str, str, ast.AST]]:\n",
    "        \"\"\"\n",
    "        Returns a list of top-level definitions as tuples:\n",
    "        (start_line, end_line, type, name, node)\n",
    "        \"\"\"\n",
    "        defs = []\n",
    "        for node in tree.body:\n",
    "            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "                defs.append(\n",
    "                    (\n",
    "                        node.lineno,\n",
    "                        getattr(node, \"end_lineno\", node.lineno),\n",
    "                        \"function\",\n",
    "                        node.name,\n",
    "                        node,\n",
    "                    )\n",
    "                )\n",
    "            elif isinstance(node, ast.ClassDef):\n",
    "                defs.append(\n",
    "                    (\n",
    "                        node.lineno,\n",
    "                        getattr(node, \"end_lineno\", node.lineno),\n",
    "                        \"class\",\n",
    "                        node.name,\n",
    "                        node,\n",
    "                    )\n",
    "                )\n",
    "        return sorted(defs, key=lambda x: x[0])\n",
    "\n",
    "    def _extract_global_intervals(\n",
    "        self,\n",
    "        code_lines: list[str],\n",
    "        top_level_defs: list[tuple[int, int, str, str, ast.AST]],\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Determines intervals (line ranges) for global code segments that are not covered by any\n",
    "        top-level definition.\n",
    "        \"\"\"\n",
    "        intervals = []\n",
    "        last_end = 1\n",
    "        for start, end, typ, name, node in top_level_defs:\n",
    "            if start > last_end:\n",
    "                intervals.append((last_end, start - 1, {\"type\": \"global\"}))\n",
    "            last_end = max(last_end, end + 1)\n",
    "        if last_end <= len(code_lines):\n",
    "            intervals.append((last_end, len(code_lines), {\"type\": \"global\"}))\n",
    "        return intervals\n",
    "\n",
    "    def _process_function(\n",
    "        self, start: int, end: int, name: str\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"Returns the interval for a function.\"\"\"\n",
    "        return [(start, end, {\"type\": \"function\", \"name\": name})]\n",
    "\n",
    "    def _process_class(\n",
    "        self, class_node: ast.ClassDef\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Processes a class node, splitting it into:\n",
    "          - Header (code before the first method),\n",
    "          - Method intervals,\n",
    "          - Gaps between methods (middle), and\n",
    "          - Footer (code after the last method).\n",
    "        \"\"\"\n",
    "        intervals = []\n",
    "        class_start = class_node.lineno\n",
    "        class_end = getattr(class_node, \"end_lineno\", class_node.lineno)\n",
    "        methods = []\n",
    "        for child in class_node.body:\n",
    "            if isinstance(child, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "                methods.append(\n",
    "                    (\n",
    "                        child.lineno,\n",
    "                        getattr(child, \"end_lineno\", child.lineno),\n",
    "                        \"method\",\n",
    "                        child.name,\n",
    "                    )\n",
    "                )\n",
    "        methods.sort(key=lambda x: x[0])\n",
    "        if methods:\n",
    "            # Class header: before the first method.\n",
    "            if class_start < methods[0][0]:\n",
    "                intervals.append(\n",
    "                    (\n",
    "                        class_start,\n",
    "                        methods[0][0] - 1,\n",
    "                        {\"type\": \"class\", \"name\": class_node.name, \"part\": \"header\"},\n",
    "                    )\n",
    "                )\n",
    "            prev_end = methods[0][1]\n",
    "            intervals.append(\n",
    "                (\n",
    "                    methods[0][0],\n",
    "                    methods[0][1],\n",
    "                    {\"type\": \"method\", \"class\": class_node.name, \"name\": methods[0][3]},\n",
    "                )\n",
    "            )\n",
    "            for i in range(1, len(methods)):\n",
    "                # Gap between methods.\n",
    "                if prev_end + 1 < methods[i][0]:\n",
    "                    intervals.append(\n",
    "                        (\n",
    "                            prev_end + 1,\n",
    "                            methods[i][0] - 1,\n",
    "                            {\n",
    "                                \"type\": \"class\",\n",
    "                                \"name\": class_node.name,\n",
    "                                \"part\": \"middle\",\n",
    "                            },\n",
    "                        )\n",
    "                    )\n",
    "                intervals.append(\n",
    "                    (\n",
    "                        methods[i][0],\n",
    "                        methods[i][1],\n",
    "                        {\n",
    "                            \"type\": \"method\",\n",
    "                            \"class\": class_node.name,\n",
    "                            \"name\": methods[i][3],\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "                prev_end = methods[i][1]\n",
    "            # Class footer: after the last method.\n",
    "            if prev_end < class_end:\n",
    "                intervals.append(\n",
    "                    (\n",
    "                        prev_end + 1,\n",
    "                        class_end,\n",
    "                        {\"type\": \"class\", \"name\": class_node.name, \"part\": \"footer\"},\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            intervals.append(\n",
    "                (class_start, class_end, {\"type\": \"class\", \"name\": class_node.name})\n",
    "            )\n",
    "        return intervals\n",
    "\n",
    "    def _extract_defs_intervals(\n",
    "        self, tree: ast.AST\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Processes all top-level definitions (functions and classes) and returns their intervals.\n",
    "        \"\"\"\n",
    "        intervals = []\n",
    "        top_level_defs = self._get_top_level_defs(tree)\n",
    "        for start, end, typ, name, node in top_level_defs:\n",
    "            if typ == \"function\":\n",
    "                intervals.extend(self._process_function(start, end, name))\n",
    "            elif typ == \"class\":\n",
    "                intervals.extend(self._process_class(node))\n",
    "        return intervals\n",
    "\n",
    "    def _create_documents_from_intervals(\n",
    "        self, code_lines: list[str], intervals: list[tuple[int, int, dict[str, Any]]]\n",
    "    ) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Converts each interval (chunk) into a Haystack Document with rich metadata.\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        chunk_number = 1\n",
    "        for start, end, meta in intervals:\n",
    "            if start > end or start < 1 or end > len(code_lines):\n",
    "                continue\n",
    "            chunk_lines = code_lines[start - 1 : end]  # Convert to 0-indexed.\n",
    "            content = \"\\n\".join(chunk_lines)\n",
    "            first_line = chunk_lines[0] if chunk_lines else \"\"\n",
    "            meta_updated = meta.copy()\n",
    "            meta_updated.update(\n",
    "                {\n",
    "                    \"chunk_number\": chunk_number,\n",
    "                    \"first_line\": first_line,\n",
    "                    \"start_line\": start,\n",
    "                    \"end_line\": end,\n",
    "                }\n",
    "            )\n",
    "            documents.append(Document(content=content, meta=meta_updated))\n",
    "            chunk_number += 1\n",
    "        return documents\n",
    "\n",
    "    def extract_documents(self) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Orchestrates the extraction process:\n",
    "          1. Reads the file and splits it into lines.\n",
    "          2. Parses the code into an AST.\n",
    "          3. Extracts intervals for definitions and global code.\n",
    "          4. Converts intervals into Haystack Documents.\n",
    "        \"\"\"\n",
    "        code = self.file_path.read_text(encoding=\"utf-8\")\n",
    "        code_lines = code.splitlines()\n",
    "        try:\n",
    "            tree = self._parse_code(code)\n",
    "        except Exception as e:\n",
    "            # If parsing fails, return the whole file as a single chunk with an error.\n",
    "            return [\n",
    "                Document(\n",
    "                    content=code,\n",
    "                    meta={\n",
    "                        \"error\": str(e),\n",
    "                        \"chunk_number\": 1,\n",
    "                        \"first_line\": code_lines[0] if code_lines else \"\",\n",
    "                        \"start_line\": 1,\n",
    "                        \"end_line\": len(code_lines),\n",
    "                    },\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        # Get intervals for definitions and global code.\n",
    "        defs_intervals = self._extract_defs_intervals(tree)\n",
    "        top_level_defs = self._get_top_level_defs(tree)\n",
    "        global_intervals = self._extract_global_intervals(code_lines, top_level_defs)\n",
    "        intervals = defs_intervals + global_intervals\n",
    "        intervals.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "        # Create documents from intervals.\n",
    "        documents = self._create_documents_from_intervals(code_lines, intervals)\n",
    "        return documents\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = Path(\"/home/pmn/repos/coder/src/coder/config/coder_paths.py\")\n",
    "    extractor = CodeFileDocumentExtractor(file_path)\n",
    "    documents = extractor.extract_documents()\n",
    "    for doc in documents:\n",
    "        print(\n",
    "            f\"Chunk {doc.meta['chunk_number']} (lines {doc.meta['start_line']}-{doc.meta['end_line']}):\"\n",
    "        )\n",
    "        print(f\"First line: {doc.meta['first_line']}\")\n",
    "        print(f\"Content:\\n{doc.content}\")\n",
    "        print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
