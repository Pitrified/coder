{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from haystack import Document, Pipeline, component\n",
    "\n",
    "# Note: the following requires a \"pip install sentence-transformers\"\n",
    "from haystack.components.embedders import (\n",
    "    SentenceTransformersDocumentEmbedder,\n",
    "    SentenceTransformersTextEmbedder,\n",
    ")\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.dataclasses import Document\n",
    "from haystack_integrations.components.retrievers.chroma import ChromaEmbeddingRetriever\n",
    "from haystack_integrations.document_stores.chroma import ChromaDocumentStore\n",
    "from loguru import logger as lg\n",
    "from rich import print as rprint\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from coder.config.coder_config import CoderConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CoderConfig()\n",
    "rprint(cc.paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeFileDocumentExtractor:\n",
    "    \"\"\"\n",
    "    Tool class that processes a single Python file and splits the code into several chunks.\n",
    "    Chunks are based on:\n",
    "      - Top-level functions,\n",
    "      - Top-level classes,\n",
    "      - Methods inside classes,\n",
    "      - And code outside these definitions (global code).\n",
    "\n",
    "    For each chunk, the following metadata is saved:\n",
    "      - 'chunk_number': Sequential number of the chunk.\n",
    "      - 'first_line': The first line of code in the chunk.\n",
    "      - 'start_line' and 'end_line': The boundaries in the original file.\n",
    "      - 'type': Type of the chunk (e.g. 'global', 'function', 'class', 'method').\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: Path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def _parse_code(self, code: str) -> ast.AST:\n",
    "        \"\"\"Parses the source code into an AST.\"\"\"\n",
    "        return ast.parse(code)\n",
    "\n",
    "    def _get_top_level_defs(\n",
    "        self, tree: ast.AST\n",
    "    ) -> list[tuple[int, int, str, str, ast.AST]]:\n",
    "        \"\"\"\n",
    "        Returns a list of top-level definitions as tuples:\n",
    "        (start_line, end_line, type, name, node)\n",
    "        \"\"\"\n",
    "        defs = []\n",
    "        for node in tree.body:\n",
    "            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "                defs.append(\n",
    "                    (\n",
    "                        node.lineno,\n",
    "                        getattr(node, \"end_lineno\", node.lineno),\n",
    "                        \"function\",\n",
    "                        node.name,\n",
    "                        node,\n",
    "                    )\n",
    "                )\n",
    "            elif isinstance(node, ast.ClassDef):\n",
    "                defs.append(\n",
    "                    (\n",
    "                        node.lineno,\n",
    "                        getattr(node, \"end_lineno\", node.lineno),\n",
    "                        \"class\",\n",
    "                        node.name,\n",
    "                        node,\n",
    "                    )\n",
    "                )\n",
    "        return sorted(defs, key=lambda x: x[0])\n",
    "\n",
    "    def _extract_global_intervals(\n",
    "        self,\n",
    "        code_lines: list[str],\n",
    "        top_level_defs: list[tuple[int, int, str, str, ast.AST]],\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Determines intervals (line ranges) for global code segments that are not covered by any\n",
    "        top-level definition.\n",
    "        \"\"\"\n",
    "        intervals = []\n",
    "        last_end = 1\n",
    "        for start, end, typ, name, node in top_level_defs:\n",
    "            if start > last_end:\n",
    "                intervals.append((last_end, start - 1, {\"type\": \"global\"}))\n",
    "            last_end = max(last_end, end + 1)\n",
    "        if last_end <= len(code_lines):\n",
    "            intervals.append((last_end, len(code_lines), {\"type\": \"global\"}))\n",
    "        return intervals\n",
    "\n",
    "    def _process_function(\n",
    "        self, start: int, end: int, name: str\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"Returns the interval for a function.\"\"\"\n",
    "        return [(start, end, {\"type\": \"function\", \"name\": name})]\n",
    "\n",
    "    def _process_class(\n",
    "        self, class_node: ast.ClassDef\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Processes a class node, splitting it into:\n",
    "          - Header (code before the first method),\n",
    "          - Method intervals,\n",
    "          - Gaps between methods (middle), and\n",
    "          - Footer (code after the last method).\n",
    "        \"\"\"\n",
    "        intervals = []\n",
    "        class_start = class_node.lineno\n",
    "        class_end = getattr(class_node, \"end_lineno\", class_node.lineno)\n",
    "        methods = []\n",
    "        for child in class_node.body:\n",
    "            if isinstance(child, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "                methods.append(\n",
    "                    (\n",
    "                        child.lineno,\n",
    "                        getattr(child, \"end_lineno\", child.lineno),\n",
    "                        \"method\",\n",
    "                        child.name,\n",
    "                    )\n",
    "                )\n",
    "        methods.sort(key=lambda x: x[0])\n",
    "        if methods:\n",
    "            # Class header: before the first method.\n",
    "            if class_start < methods[0][0]:\n",
    "                intervals.append(\n",
    "                    (\n",
    "                        class_start,\n",
    "                        methods[0][0] - 1,\n",
    "                        {\"type\": \"class\", \"name\": class_node.name, \"part\": \"header\"},\n",
    "                    )\n",
    "                )\n",
    "            prev_end = methods[0][1]\n",
    "            intervals.append(\n",
    "                (\n",
    "                    methods[0][0],\n",
    "                    methods[0][1],\n",
    "                    {\"type\": \"method\", \"class\": class_node.name, \"name\": methods[0][3]},\n",
    "                )\n",
    "            )\n",
    "            for i in range(1, len(methods)):\n",
    "                # Gap between methods.\n",
    "                if prev_end + 1 < methods[i][0]:\n",
    "                    intervals.append(\n",
    "                        (\n",
    "                            prev_end + 1,\n",
    "                            methods[i][0] - 1,\n",
    "                            {\n",
    "                                \"type\": \"class\",\n",
    "                                \"name\": class_node.name,\n",
    "                                \"part\": \"middle\",\n",
    "                            },\n",
    "                        )\n",
    "                    )\n",
    "                intervals.append(\n",
    "                    (\n",
    "                        methods[i][0],\n",
    "                        methods[i][1],\n",
    "                        {\n",
    "                            \"type\": \"method\",\n",
    "                            \"class\": class_node.name,\n",
    "                            \"name\": methods[i][3],\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "                prev_end = methods[i][1]\n",
    "            # Class footer: after the last method.\n",
    "            if prev_end < class_end:\n",
    "                intervals.append(\n",
    "                    (\n",
    "                        prev_end + 1,\n",
    "                        class_end,\n",
    "                        {\"type\": \"class\", \"name\": class_node.name, \"part\": \"footer\"},\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            intervals.append(\n",
    "                (class_start, class_end, {\"type\": \"class\", \"name\": class_node.name})\n",
    "            )\n",
    "        return intervals\n",
    "\n",
    "    def _extract_defs_intervals(\n",
    "        self, tree: ast.AST\n",
    "    ) -> list[tuple[int, int, dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Processes all top-level definitions (functions and classes) and returns their intervals.\n",
    "        \"\"\"\n",
    "        intervals = []\n",
    "        top_level_defs = self._get_top_level_defs(tree)\n",
    "        for start, end, typ, name, node in top_level_defs:\n",
    "            if typ == \"function\":\n",
    "                intervals.extend(self._process_function(start, end, name))\n",
    "            elif typ == \"class\":\n",
    "                intervals.extend(self._process_class(node))\n",
    "        return intervals\n",
    "\n",
    "    def _create_documents_from_intervals(\n",
    "        self, code_lines: list[str], intervals: list[tuple[int, int, dict[str, Any]]]\n",
    "    ) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Converts each interval (chunk) into a Haystack Document with rich metadata.\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        chunk_number = 1\n",
    "        for start, end, meta in intervals:\n",
    "            if start > end or start < 1 or end > len(code_lines):\n",
    "                continue\n",
    "            chunk_lines = code_lines[start - 1 : end]  # Convert to 0-indexed.\n",
    "            content = \"\\n\".join(chunk_lines)\n",
    "            first_line = chunk_lines[0] if chunk_lines else \"\"\n",
    "            meta_updated = meta.copy()\n",
    "            meta_updated.update(\n",
    "                {\n",
    "                    \"chunk_number\": chunk_number,\n",
    "                    \"first_line\": first_line,\n",
    "                    \"start_line\": start,\n",
    "                    \"end_line\": end,\n",
    "                    \"file_path\": str(self.file_path),\n",
    "                }\n",
    "            )\n",
    "            # skip empty chunks\n",
    "            if not content.strip():\n",
    "                lines = f\"lines {start}-{end}\"\n",
    "                lg.debug(f\"Skipping empty chunk {chunk_number} ({lines})\")\n",
    "                # chunk_number += 1\n",
    "                continue\n",
    "            documents.append(Document(content=content, meta=meta_updated))\n",
    "            chunk_number += 1\n",
    "        return documents\n",
    "\n",
    "    def extract_documents(self) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Orchestrates the extraction process:\n",
    "          1. Reads the file and splits it into lines.\n",
    "          2. Parses the code into an AST.\n",
    "          3. Extracts intervals for definitions and global code.\n",
    "          4. Converts intervals into Haystack Documents.\n",
    "        \"\"\"\n",
    "        code = self.file_path.read_text(encoding=\"utf-8\")\n",
    "        code_lines = code.splitlines()\n",
    "        try:\n",
    "            tree = self._parse_code(code)\n",
    "        except Exception as e:\n",
    "            lg.warning(f\"Failed to parse {self.file_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "        # Get intervals for definitions and global code.\n",
    "        defs_intervals = self._extract_defs_intervals(tree)\n",
    "        top_level_defs = self._get_top_level_defs(tree)\n",
    "        global_intervals = self._extract_global_intervals(code_lines, top_level_defs)\n",
    "        intervals = defs_intervals + global_intervals\n",
    "        intervals.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "        # Create documents from intervals.\n",
    "        documents = self._create_documents_from_intervals(code_lines, intervals)\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class CodeFolderIngestor:\n",
    "    \"\"\"\n",
    "    Pipeline component that accepts a list of folder paths, uses the CodeFileDocumentExtractor\n",
    "    for each Python file found in each folder, and aggregates the resulting Haystack Documents.\n",
    "    \"\"\"\n",
    "\n",
    "    @component.output_types(documents=list[Document])\n",
    "    def run(self, folder_paths: list[Path]) -> dict[str, list[Document]]:\n",
    "        documents: list[Document] = []\n",
    "        for folder_path in folder_paths:\n",
    "            for file_path in folder_path.rglob(\"*.py\"):\n",
    "                extractor = CodeFileDocumentExtractor(file_path)\n",
    "                document = extractor.extract_documents()\n",
    "                documents.extend(document)\n",
    "        return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [cc.paths.src_fol]\n",
    "ingestor = CodeFolderIngestor()\n",
    "result = ingestor.run(folders)\n",
    "documents = result[\"documents\"]\n",
    "print(f\"Ingested {len(documents)} documents from the provided folder paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].content[:100], documents[2].meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = ChromaDocumentStore(\n",
    "    # persist_path=str(cc.paths.chroma_fol),\n",
    ")\n",
    "\n",
    "# document_store.write_documents(documents=documents)\n",
    "# print(document_store.count_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing = Pipeline()\n",
    "indexing.add_component(\"embedder\", SentenceTransformersDocumentEmbedder())\n",
    "indexing.add_component(\"writer\", DocumentWriter(document_store))\n",
    "indexing.connect(\"embedder.documents\", \"writer.documents\")\n",
    "indexing.run({\"embedder\": {\"documents\": documents}})\n",
    "print(document_store.count_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample retriever\n",
    "\n",
    "retriever = ChromaEmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "# example run query\n",
    "retriever.run(query_embedding=[0.1] * 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample retrieve embedding the query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querying = Pipeline()\n",
    "querying.add_component(\"query_embedder\", SentenceTransformersTextEmbedder())\n",
    "querying.add_component(\"retriever\", ChromaEmbeddingRetriever(document_store))\n",
    "querying.connect(\"query_embedder.embedding\", \"retriever.query_embedding\")\n",
    "results = querying.run({\"query_embedder\": {\"text\": \"Variable declarations\"}})\n",
    "\n",
    "for d in results[\"retriever\"][\"documents\"]:\n",
    "    print(d.meta, d.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever expanded context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Dict, Any, Set, Tuple\n",
    "# from haystack import component\n",
    "# from haystack import Document\n",
    "\n",
    "\n",
    "@component\n",
    "class ContextRetriever:\n",
    "    \"\"\"\n",
    "    Retriever component that enhances a list of retrieved documents by fetching\n",
    "    additional context documents from a Chroma store. The context is determined based\n",
    "    on the 'chunk_number' and 'file_path' metadata, retrieving documents whose\n",
    "    chunk_number is within a specified window before and after the original chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        chroma_store,\n",
    "        context_window_before: int = 1,\n",
    "        context_window_after: int = 3,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param chroma_store: An instance of a Chroma document store that supports filter_documents.\n",
    "        :param context_window: Number of chunks to retrieve before and after the original chunk.\n",
    "        \"\"\"\n",
    "        self.chroma_store = chroma_store\n",
    "        self.context_window_before = context_window_before\n",
    "        self.context_window_after = context_window_after\n",
    "\n",
    "    @component.output_types(documents=list[Document])\n",
    "    def run(self, retrieved_docs: list[Document]) -> dict[str, list[Document]]:\n",
    "        \"\"\"\n",
    "        For each document in retrieved_docs, query the Chroma store for additional context\n",
    "        documents based on the file_path and chunk_number metadata.\n",
    "\n",
    "        :param retrieved_docs: List of documents initially retrieved by the RAG pipeline.\n",
    "        :return: A dictionary with key 'documents' mapping to the list of context-enhanced documents.\n",
    "        \"\"\"\n",
    "        context_documents: list[Document] = []\n",
    "        seen: set[tuple[Any, Any]] = set()  # To avoid duplicate documents\n",
    "\n",
    "        for doc in retrieved_docs:\n",
    "            file_path = doc.meta.get(\"file_path\")\n",
    "            chunk_number = doc.meta.get(\"chunk_number\")\n",
    "            if file_path is None or chunk_number is None:\n",
    "                continue  # Skip documents missing essential metadata\n",
    "\n",
    "            # Define a filter to retrieve documents with a chunk_number within the context window\n",
    "\n",
    "            filters = {\n",
    "                \"operator\": \"AND\",\n",
    "                \"conditions\": [\n",
    "                    {\"field\": \"meta.file_path\", \"operator\": \"==\", \"value\": file_path},\n",
    "                    {\n",
    "                        \"field\": \"meta.chunk_number\",\n",
    "                        \"operator\": \">=\",\n",
    "                        \"value\": chunk_number - self.context_window_before,\n",
    "                    },\n",
    "                    {\n",
    "                        \"field\": \"meta.chunk_number\",\n",
    "                        \"operator\": \"<=\",\n",
    "                        \"value\": chunk_number + self.context_window_after,\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "\n",
    "            # Retrieve context documents from the Chroma store\n",
    "            docs = self.chroma_store.filter_documents(filters=filters)\n",
    "            for d in docs:\n",
    "                key = (d.meta.get(\"file_path\"), d.meta.get(\"chunk_number\"))\n",
    "                if key not in seen:\n",
    "                    seen.add(key)\n",
    "                    context_documents.append(d)\n",
    "\n",
    "        return {\"documents\": context_documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querying = Pipeline()\n",
    "querying.add_component(\"query_embedder\", SentenceTransformersTextEmbedder())\n",
    "querying.add_component(\"retriever\", ChromaEmbeddingRetriever(document_store))\n",
    "querying.connect(\"query_embedder.embedding\", \"retriever.query_embedding\")\n",
    "results = querying.run({\"query_embedder\": {\"text\": \"Variable declarations\"}})\n",
    "\n",
    "for d in results[\"retriever\"][\"documents\"]:\n",
    "    print(d.meta, d.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_retriever = ChromaEmbeddingRetriever(\n",
    "    document_store,\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "# Pass the same document store (or its chroma store) to the ContextRetriever\n",
    "context_retriever = ContextRetriever(\n",
    "    chroma_store=document_store,\n",
    "    context_window_before=1,\n",
    "    context_window_after=3,\n",
    ")\n",
    "\n",
    "# Build the pipeline\n",
    "querying = Pipeline()\n",
    "querying.add_component(\"query_embedder\", SentenceTransformersTextEmbedder())\n",
    "querying.add_component(\"retriever\", base_retriever)\n",
    "querying.add_component(\"context_retriever\", context_retriever)\n",
    "\n",
    "# Connect components:\n",
    "# Connect the query embedder to the retriever\n",
    "querying.connect(\"query_embedder.embedding\", \"retriever.query_embedding\")\n",
    "# Connect the output documents of the retriever to the context retriever\n",
    "querying.connect(\"retriever.documents\", \"context_retriever.retrieved_docs\")\n",
    "\n",
    "# Run the pipeline\n",
    "results = querying.run(\n",
    "    {\"query_embedder\": {\"text\": \"Variable declarations\"}},\n",
    "    include_outputs_from={\"retriever\", \"context_retriever\"},\n",
    ")\n",
    "\n",
    "# Retrieve original and expanded context documents\n",
    "original_docs = results[\"retriever\"][\"documents\"]\n",
    "expanded_context_docs = results[\"context_retriever\"][\"documents\"]\n",
    "\n",
    "# For debugging: print both sets\n",
    "print(\"Original Documents:\")\n",
    "for d in original_docs:\n",
    "    print(d.meta, d.score)\n",
    "\n",
    "print(\"\\nExpanded Context Documents:\")\n",
    "for d in expanded_context_docs:\n",
    "    print(d.meta, d.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
